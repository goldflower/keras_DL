{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "參考以下資源:\n",
    "\n",
    "https://github.com/keras-team/keras/blob/master/examples/neural_style_transfer.py\n",
    "\n",
    "https://github.com/robertomest/neural-style-keras\n",
    "\n",
    "原論文：https://arxiv.org/abs/1508.06576\n",
    "\n",
    "tv loss: https://arxiv.org/pdf/1412.0035.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goldflower/.pyenv/versions/3.6.3/envs/env_3.6.3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# pretrained model\n",
    "from keras.applications import vgg19\n",
    "from keras.layers import Input\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Nadam, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, img_size=None):\n",
    "    if img_size:\n",
    "        img = load_img(image_path, target_size = img_size)\n",
    "    else:\n",
    "        img = load_img(image_path)\n",
    "    img_arr = img_to_array(img)\n",
    "    img_arr = np.expand_dims(img_arr, axis=0)\n",
    "    # 套normalization, 不做scaling\n",
    "    # 若mode = 'tf'則會scale到[0-1]\n",
    "    img_arr = vgg19.preprocess_input(img_arr)\n",
    "    return img, img_arr\n",
    "\n",
    "def deprocess_image(x):\n",
    "    x = x[0]\n",
    "    # vgg19的mean\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim 3 -> 4 (batch_size, rows, cols, channels)\n",
    "base_img, base_img_arr = preprocess_image('dog.jpeg')\n",
    "width, height = base_img_arr.shape[2], base_img_arr.shape[1]\n",
    "img_nrows, img_ncols = height, width\n",
    "\n",
    "# to tensorflow tensor variable\n",
    "ref_img, ref_img_arr = preprocess_image('style_3.png')\n",
    "\n",
    "# 會改變的變數, 這就是我們想要的輸出結果\n",
    "# 我們想要：圖整體長得像base但是風格類似於ref\n",
    "# std = 0.001\n",
    "# 從random開始, 效果有點差\n",
    "# generated_img_var = K.variable(std * np.random.randn(*base_img_arr.shape))\n",
    "# 直接從base image開始\n",
    "generated_img_var = K.variable(base_img_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下來我們要定義loss fcuntion, 我們有兩種loss, 第一個為content loss, 衡量訓練出來的圖跟base長得像不像, 另一個則是style loss, 衡量訓練出來的圖style是否類似於ref\n",
    "\n",
    "首先定義content loss公式如下：\n",
    "\n",
    "<img style=\"float:center; width:310px\" src=\"content_loss.png\"/>\n",
    "\n",
    "$p$: base_img\n",
    "\n",
    "$x$: 生成的圖片\n",
    "\n",
    "$F^l$和$P^l$為一個$N*M$的矩陣, 其中包含了$p$和$x$的feature representation in layer $l$, $N$為kernel(filter)的數目而$M$為此kernel中的大小(比如5*7 kernel, 則$M$等於35)\n",
    "\n",
    "如此一來, 上式即可衡量base以及生成的圖像的差距, 並利用BP來訓練\n",
    "\n",
    "接下來我們要定義style loss, 這裡並沒有使用類似上述的方式來對feature map值的差距當作loss, 因為我們並沒有要讓生成圖片和ref在\"數值上\"相似, 而是在style上相似, 論文中利用gram matrix來當作衡量style的依據, 首先定義gram matrix G, 並且每個元素定義如下:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img alt=\"Drawing\" style=\"width:150px\" src=\"gram_matrix.png\"/></td>\n",
    "        <td><img alt=\"Drawing\" style=\"width:150px\" src=\"gram_matrix_1.png\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "意義在於對於每個row $i$, 第j個值代表了這個row與第j個row的內積, 這代表了我們把各個kernel間的相似程度當作一張圖片總體的style, 每個feature map想像其隱含某種style的資訊, 那隱含差不多style資訊的值應該會比較高, 則G在這點的值也會比較大, 而某個feature map的值原本就很小也就代表了雖然具有某種style, 但並不強烈。\n",
    "\n",
    "定義完$G$後我們就能定義layer $l$的loss如下：\n",
    "\n",
    "<img style=\"float:center; width:300px\" src=\"style_layer_loss.png\"/>\n",
    "\n",
    "$A$, $G$分別為ref和生成後的gram matrix, $N$和$M$的定義與上面相同\n",
    "\n",
    "因此我們定義total style loss如下:\n",
    "\n",
    "<img style=\"float:center; width:300px\" src=\"style_loss.png\"/>\n",
    "\n",
    "在這邊$w_l$為自己手動設定的權重, 也就是一個超參數, 我們可以對每一層的loss都去使用不同的權重, 看自己覺得哪個重要, 然後這個部分的微分式如\n",
    "\n",
    "![style gradient](style_gradient.png)\n",
    "\n",
    "最後我們就能得到總體的loss如下：\n",
    "![loss](loss_func.png)\n",
    "\n",
    "論文中的設計只到以上, 但實際上要得到更好的效果, 會再加入total variation error, 此error的形式如：\n",
    "![tv_loss](tv_loss.png)\n",
    "此式的意義在於我們同時希望鄰近的pixel間其差距要越低越好, 亦即希望結果smooth\n",
    "\n",
    "綜合以上, 我們可以寫程式如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    # 第三維是kernel數, 整理成(nb_kernels x 剩下的)\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    # 變成squared, 之後算error要用\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    # normalize for better result, optional\n",
    "    norm = x.shape\n",
    "    normalize_term = norm[1] * norm[2]\n",
    "    gram = gram / K.cast(normalize_term, x.dtype)\n",
    "    return gram\n",
    "\n",
    "def style_loss(style, generated):\n",
    "    # style loss為兩張圖片彼此比較\n",
    "    A = gram_matrix(style)\n",
    "    G = gram_matrix(generated)\n",
    "    channels = 3\n",
    "    size = img_nrows * img_ncols\n",
    "    return K.sum(K.square(G - A)) / (4. * (channels ** 2) * (size ** 2))\n",
    "\n",
    "def content_loss(base, generated): \n",
    "    # content loss為兩張圖片彼此比較    \n",
    "    return K.sum(K.square(generated - base))\n",
    "\n",
    "def total_variation_loss(x, beta=2.5):\n",
    "    assert K.ndim(x) == 3\n",
    "    a = K.square(x[:img_nrows-1, 1:img_ncols, :] - x[:img_nrows-1, :img_ncols-1, :]) # 上述tv loss第一項\n",
    "    b = K.square(x[1:img_nrows, :img_ncols-1, :] - x[:img_nrows-1, :img_ncols-1, :]) # tv loss第二項\n",
    "    return K.sum(K.pow(a+b, beta/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_weight = 0.001 # alpha\n",
    "style_weight = 50000 # beta\n",
    "total_variation_weight = 1e-4 # tv loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們前面的網路利用VGG19來提取圖像資訊\n",
    "\n",
    "VGG19的架構如下\n",
    "\n",
    "<img style=\"float:center width:200x\" src=\"vgg19.png\"/>\n",
    "\n",
    "每個相同大小kernel的區段稱為一個block, 因此可以看出其在FC之前有5個block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include_top: 最後flatten那幾層FC要不要讀?\n",
    "# 因為我們只要抽出feature map的部分，所以不用\n",
    "# 首先先算base和ref的輸出值, 這個部分的結果不會改變, 所以叫做static\n",
    "static_model = vgg19.VGG19(weights='imagenet', include_top=False)\n",
    "outputs_dict = {layer.name: layer.output for layer in static_model.layers}\n",
    "\n",
    "# Q: 為何用block5_conv2而不是block5_conv4?  \n",
    "# A: 沒說, 目前想不到原因, 感覺純粹是實驗結果好, 不然以論文中所說, 應該要盡量用抽象資訊才對\n",
    "# Q: 為何只取一個block不取全部?\n",
    "# A: 論文貌似也沒說明, 感覺上是因為這層足夠抽象(但又回到為啥不取block5_conv4了QQ\")\n",
    "content_feature_layers = ['block5_conv2']\n",
    "# Q: 為何都用conv1?\n",
    "# A: 因為我們通常會認為較淺層保留偏向局部的資訊, 而較深層含有更抽象的資訊\n",
    "#    因此我們也認為淺層比較能夠代表style (我猜的)\n",
    "style_feature_layers = ['block1_conv1', 'block2_conv1',\n",
    "                        'block3_conv1', 'block4_conv1',\n",
    "                        'block5_conv1']\n",
    "\n",
    "content_features = [outputs_dict[layer] for layer in content_feature_layers]\n",
    "style_features = [outputs_dict[layer] for layer in style_feature_layers]\n",
    "\n",
    "# 輸入input給model, 得到指定層的output(這些output根據輸入而變化)\n",
    "get_content_fun = K.function([static_model.input], content_features)\n",
    "get_style_fun = K.function([static_model.input], style_features)\n",
    "# 輸入content和style, 分別得到想要的層的結果, 用來算loss\n",
    "content_targets = get_content_fun([base_img_arr])\n",
    "style_targets = get_style_fun([ref_img_arr])\n",
    "\n",
    "# 輸出的值要轉成variable才能計算error\n",
    "content_targets_dict = {k: K.variable(v) for k, v in zip(content_feature_layers, content_targets)}\n",
    "style_targets_dict = {k: K.variable(v) for k, v in zip(style_feature_layers, style_targets)}\n",
    "# 這是要訓練的NN\n",
    "trainable_model = vgg19.VGG19(weights='imagenet', include_top=False,\n",
    "                              input_tensor=Input(tensor=generated_img_var))\n",
    "generated_outputs_dict = {layer.name: layer.output for layer in trainable_model.layers}\n",
    "\n",
    "total_loss = K.variable(0.)\n",
    "total_content_loss = K.variable(0.)\n",
    "total_style_loss = K.variable(0.)\n",
    "total_tv_loss = K.variable(0.)\n",
    "# add content loss\n",
    "for layer_name in content_feature_layers:\n",
    "    layer_feature = content_targets_dict[layer_name]\n",
    "    gen_feature = generated_outputs_dict[layer_name]\n",
    "    l = content_loss(layer_feature, gen_feature)\n",
    "    weighted_content_loss = content_weight * l\n",
    "    total_content_loss = total_content_loss + weighted_content_loss\n",
    "    total_loss = total_loss + weighted_content_loss\n",
    "# add style loss\n",
    "for layer_name in style_feature_layers:\n",
    "    # [0] 因為三維比較好理解(可以直接套上面公式), \n",
    "    # 第0維可以代表batch size, 如果要一次做多種style\n",
    "    # 那就要改一下這邊來得到多組style error\n",
    "    layer_feature = style_targets_dict[layer_name][0]\n",
    "    gen_feature = generated_outputs_dict[layer_name][0]\n",
    "    l = style_loss(layer_feature, gen_feature)\n",
    "    weighted_style_loss = style_weight * l\n",
    "    total_style_loss = total_style_loss + weighted_style_loss\n",
    "    total_loss = total_loss + weighted_style_loss\n",
    "# add tv loss\n",
    "weighted_tv_loss = total_variation_weight * total_variation_loss(generated_img_var[0])\n",
    "total_tv_loss = total_tv_loss + weighted_tv_loss\n",
    "total_loss = total_loss + weighted_tv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Nadam(10)\n",
    "#opt = Adam(10)\n",
    "updates = opt.get_updates([generated_img_var], {}, total_loss)\n",
    "# List of outputs\n",
    "outputs = [total_loss, total_content_loss, total_style_loss, total_tv_loss]\n",
    "# Function that makes a step after backpropping to the image\n",
    "make_step = K.function([], outputs, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Content loss at pass10: 521307\n",
      " Style loss at pass10: 2.1361e+06\n",
      " TV loss at pass10: 247952\n",
      " Total loss at pass10: 2.90536e+06\n",
      " Content loss at pass20: 539696\n",
      " Style loss at pass20: 2.75188e+06\n",
      " TV loss at pass20: 271838\n",
      " Total loss at pass20: 3.56342e+06\n",
      " Content loss at pass30: 422104\n",
      " Style loss at pass30: 2.45959e+06\n",
      " TV loss at pass30: 205711\n",
      " Total loss at pass30: 3.08741e+06\n",
      " Content loss at pass40: 403624\n",
      " Style loss at pass40: 2.03165e+06\n",
      " TV loss at pass40: 229554\n",
      " Total loss at pass40: 2.66483e+06\n",
      " Content loss at pass50: 332097\n",
      " Style loss at pass50: 947925\n",
      " TV loss at pass50: 187509\n",
      " Total loss at pass50: 1.46753e+06\n",
      " Content loss at pass60: 319558\n",
      " Style loss at pass60: 1.16925e+06\n",
      " TV loss at pass60: 157461\n",
      " Total loss at pass60: 1.64626e+06\n",
      " Content loss at pass70: 699168\n",
      " Style loss at pass70: 7.26295e+06\n",
      " TV loss at pass70: 417066\n",
      " Total loss at pass70: 8.37919e+06\n",
      " Content loss at pass80: 466268\n",
      " Style loss at pass80: 2.17011e+06\n",
      " TV loss at pass80: 362382\n",
      " Total loss at pass80: 2.99876e+06\n",
      " Content loss at pass90: 339478\n",
      " Style loss at pass90: 1.37461e+06\n",
      " TV loss at pass90: 234895\n",
      " Total loss at pass90: 1.94899e+06\n",
      " Content loss at pass100: 353192\n",
      " Style loss at pass100: 1.29303e+06\n",
      " TV loss at pass100: 177628\n",
      " Total loss at pass100: 1.82384e+06\n",
      " Content loss at pass110: 346044\n",
      " Style loss at pass110: 1.72713e+06\n",
      " TV loss at pass110: 189303\n",
      " Total loss at pass110: 2.26248e+06\n",
      " Content loss at pass120: 327715\n",
      " Style loss at pass120: 1.39016e+06\n",
      " TV loss at pass120: 165258\n",
      " Total loss at pass120: 1.88313e+06\n",
      " Content loss at pass130: 280424\n",
      " Style loss at pass130: 901774\n",
      " TV loss at pass130: 163849\n",
      " Total loss at pass130: 1.34605e+06\n",
      " Content loss at pass140: 253212\n",
      " Style loss at pass140: 751235\n",
      " TV loss at pass140: 152375\n",
      " Total loss at pass140: 1.15682e+06\n",
      " Content loss at pass150: 266670\n",
      " Style loss at pass150: 982786\n",
      " TV loss at pass150: 155076\n",
      " Total loss at pass150: 1.40453e+06\n",
      " Content loss at pass160: 625855\n",
      " Style loss at pass160: 4.42445e+06\n",
      " TV loss at pass160: 329138\n",
      " Total loss at pass160: 5.37944e+06\n",
      " Content loss at pass170: 315979\n",
      " Style loss at pass170: 1.4e+06\n",
      " TV loss at pass170: 261084\n",
      " Total loss at pass170: 1.97707e+06\n",
      " Content loss at pass180: 283828\n",
      " Style loss at pass180: 886527\n",
      " TV loss at pass180: 204977\n",
      " Total loss at pass180: 1.37533e+06\n",
      " Content loss at pass190: 233147\n",
      " Style loss at pass190: 517872\n",
      " TV loss at pass190: 160420\n",
      " Total loss at pass190: 911438\n",
      " Content loss at pass200: 288776\n",
      " Style loss at pass200: 905338\n",
      " TV loss at pass200: 165126\n",
      " Total loss at pass200: 1.35924e+06\n",
      " Content loss at pass210: 323355\n",
      " Style loss at pass210: 1.25556e+06\n",
      " TV loss at pass210: 209183\n",
      " Total loss at pass210: 1.7881e+06\n",
      " Content loss at pass220: 234082\n",
      " Style loss at pass220: 640457\n",
      " TV loss at pass220: 169580\n",
      " Total loss at pass220: 1.04412e+06\n",
      " Content loss at pass230: 234967\n",
      " Style loss at pass230: 530343\n",
      " TV loss at pass230: 154003\n",
      " Total loss at pass230: 919314\n",
      " Content loss at pass240: 205205\n",
      " Style loss at pass240: 553892\n",
      " TV loss at pass240: 134171\n",
      " Total loss at pass240: 893269\n",
      " Content loss at pass250: 339167\n",
      " Style loss at pass250: 1.74696e+06\n",
      " TV loss at pass250: 173450\n",
      " Total loss at pass250: 2.25957e+06\n",
      " Content loss at pass260: 536519\n",
      " Style loss at pass260: 2.26922e+06\n",
      " TV loss at pass260: 257401\n",
      " Total loss at pass260: 3.06314e+06\n"
     ]
    }
   ],
   "source": [
    "# 如果效果不好, 根據error的值回去調整alpha/beta/tv weight\n",
    "max_iters = 1000\n",
    "for i in range(max_iters):\n",
    "    out = make_step([])\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(' Content loss at pass{}: %g'.format(i+1) %out[1])\n",
    "        print(' Style loss at pass{}: %g'.format(i+1) %out[2])\n",
    "        print(' TV loss at pass{}: %g'.format(i+1) %out[3])\n",
    "        print(' Total loss at pass{}: %g'.format(i+1) %out[0])\n",
    "        x = K.get_value(generated_img_var)        \n",
    "        fname = 'results/{}.png'.format(i+1)\n",
    "        img = deprocess_image(x)\n",
    "        img = Image.fromarray(img)\n",
    "        img.save(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下圖為實驗結果, 左邊為base, 右邊為ref\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img style=\"float:left; width:310px\" src=\"dog.jpeg\"/> </td>\n",
    "        <td> <img style=\"float:right; width:310px\" src=\"style_3.png\"/> </td> \n",
    "    </tr>\n",
    "</table>\n",
    "我們用上述設定, 經過250次iteration後得到\n",
    "\n",
    "<img style=\"float:center; width:310px\" src=\"250.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
