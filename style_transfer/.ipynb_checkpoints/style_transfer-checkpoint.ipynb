{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "參考以下資源:\n",
    "\n",
    "https://github.com/keras-team/keras/blob/master/examples/neural_style_transfer.py\n",
    "\n",
    "https://github.com/robertomest/neural-style-keras\n",
    "\n",
    "原論文：https://arxiv.org/abs/1508.06576\n",
    "\n",
    "tv loss參考自: https://arxiv.org/pdf/1412.0035.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goldflower/.pyenv/versions/3.6.3/envs/env_3.6.3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# pretrained model\n",
    "from keras.applications import vgg19\n",
    "from keras.layers import Input\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = base_img.size # 輸出圖片size等於base_img的大小\n",
    "img_nrows, img_ncols = height, width # 懶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    # 全部減去均值, 不做scaling, 並且會從RGB轉為BGR\n",
    "    # 若mode='tf', 則會做scaling至[-1, 1]    \n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x):\n",
    "    x = x.reshape((img_nrows, img_ncols, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    # 這是imagenet的設定\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 898, 879, 3)\n",
      "(1, 898, 879, 3)\n",
      "Tensor(\"Placeholder:0\", shape=(1, 898, 879, 3), dtype=float32)\n",
      "<tf.Variable 'Variable_2:0' shape=(1, 898, 879, 3) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "# dim 3 -> 4 (batch_size, rows, cols, channels)\n",
    "base_img_arr = preprocess_image('loss.jpg')\n",
    "width, height = base_img_arr.shape[2], base_img_arr.shape[1] \n",
    "img_nrows, img_ncols = height, width # 懶, 直接用一樣size\n",
    "# to tensorflow tensor variable\n",
    "base_img_var = K.variable(base_img_arr)\n",
    "\n",
    "ref_img_arr = nbase_img_arr = preprocess_image('star.jpg')\n",
    "ref_img_var = K.variable(ref_img_arr)\n",
    "\n",
    "# 會改變的變數, 這就是我們想要的輸出結果\n",
    "# 我們想要：圖整體長得像base但是風格類似於ref\n",
    "generated_img_var = K.variable(np.random.randn(1, *base_img_arr.shape[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下來我們要定義loss fcuntion, 我們有兩種loss, 第一個為content loss, 衡量訓練出來的圖跟base長得像不像, 另一個則是style loss, 衡量訓練出來的圖style是否類似於ref\n",
    "\n",
    "首先定義content loss公式如下：\n",
    "\n",
    "<img style=\"float:center; width:310px\" src=\"content_loss.png\"/>\n",
    "\n",
    "$p$: base_img\n",
    "\n",
    "$x$: 生成的圖片\n",
    "\n",
    "$F^l$和$P^l$為一個$N*M$的矩陣, 其中包含了$p$和$x$的feature representation in layer $l$, $N$為kernel(filter)的數目而$M$為此kernel中的大小(比如5*7 kernel, 則$M$等於35)\n",
    "\n",
    "如此一來, 上式即可衡量base以及生成的圖像的差距, 並利用BP來訓練\n",
    "\n",
    "接下來我們要定義style loss, 這裡並沒有使用類似上述的方式來對feature map值的差距當作loss, 因為我們並沒有要讓生成圖片和ref在\"數值上\"相似, 而是在style上相似, 論文中利用gram matrix來當作衡量style的依據, 首先定義gram matrix G, 並且每個元素定義如下:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img alt=\"Drawing\" style=\"width:150px\" src=\"gram_matrix.png\"/></td>\n",
    "        <td><img alt=\"Drawing\" style=\"width:150px\" src=\"gram_matrix_1.png\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "意義在於對於每個row $i$, 第j個值代表了這個row與第j個row的內積, 這代表了我們把各個kernel間的相似程度當作一張圖片總體的style, 每個feature map想像其隱含某種style的資訊, 那隱含差不多style資訊的值應該會比較高, 則G在這點的值也會比較大, 而某個feature map的值原本就很小也就代表了雖然具有某種style, 但並不強烈。\n",
    "\n",
    "定義完$G$後我們就能定義layer $l$的loss如下：\n",
    "\n",
    "<img style=\"float:center; width:300px\" src=\"style_layer_loss.png\"/>\n",
    "\n",
    "$A$, $G$分別為ref和生成後的gram matrix, $N$和$M$的定義與上面相同\n",
    "\n",
    "因此我們定義total style loss如下:\n",
    "\n",
    "<img style=\"float:center; width:300px\" src=\"style_loss.png\"/>\n",
    "\n",
    "在這邊$w_l$為自己手動設定的權重, 也就是一個超參數, 我們可以對每一層的loss都去使用不同的權重, 看自己覺得哪個重要, 然後這個部分的微分式如\n",
    "\n",
    "![style gradient](style_gradient.png)\n",
    "\n",
    "最後我們就能得到總體的loss如下：\n",
    "![loss](loss_func.png)\n",
    "\n",
    "論文中的設計只到以上, 但實際上要得到更好的效果, 會再加入total variation error, 此error的形式如：\n",
    "![tv_loss](tv_loss.png)\n",
    "此式的意義在於我們同時希望鄰近的pixel間其差距要越低越好, 亦即希望結果smooth\n",
    "\n",
    "綜合以上, 我們可以寫程式如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    assert K.ndim(x) == 3\n",
    "    # batch_flatten: 變成 n_samples x n_features的2維tensor\n",
    "    features = K.batch_flatten(x)\n",
    "    # 變成squared, 之後算error要用\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram\n",
    "\n",
    "def style_loss(style, generated):\n",
    "    # style loss為兩張圖片彼此比較\n",
    "    assert K.ndim(style) == 3\n",
    "    assert K.ndim(generated) == 3\n",
    "    A = gram_matrix(style)\n",
    "    G = gram_matrix(generated)\n",
    "    channels = 3\n",
    "    size = img_nrows * img_ncols\n",
    "    return K.sum(K.square(G - A) / (4. * (channels ** 2) * (size ** 2)))\n",
    "\n",
    "def content_loss(base, generated):\n",
    "    assert K.ndim(base) == 3\n",
    "    assert K.ndim(generated) == 3    \n",
    "    # content loss為兩張圖片彼此比較    \n",
    "    return K.sum(K.square(generated - base))\n",
    "\n",
    "def total_variation_loss(x, beta=2.5):\n",
    "    assert K.ndim(x) == 3\n",
    "    a = K.square(x[:img_nrows-1, 1:img_ncols, :] - x[:img_nrows-1, :img_ncols-1, :]) # 上述tv loss第一項\n",
    "    b = K.square(x[1:img_nrows, :img_ncols-1, :] - x[:img_nrows-1, :img_ncols-1, :]) # tv loss第二項\n",
    "    return K.sum(K.pow(a+b, beta/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_weight = 1 # alpha\n",
    "style_weight = 1e-4 # beta\n",
    "total_variation_weight = 1e-4 # tv loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們前面的網路利用VGG19來提取圖像資訊\n",
    "\n",
    "VGG19的架構如下\n",
    "\n",
    "<img style=\"float:center width:200x\" src=\"vgg19.png\"/>\n",
    "\n",
    "每個相同大小kernel的區段稱為一個block, 因此可以看出其在FC之前有5個block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_21:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# include_top: 最後flatten那幾層FC要不要讀?\n",
    "# 因為我們只要抽出feature map的部分，所以不用\n",
    "# 首先先算base和ref的輸出值, 這個部分的結果不會改變, 所以叫做static\n",
    "static_model = vgg19.VGG19(weights='imagenet', include_top=False)\n",
    "outputs_dict = {layer.name: layer.output for layer in static_model.layers}\n",
    "\n",
    "# Q: 為何用block5_conv2而不是block5_conv4?  \n",
    "# A: 沒說, 目前想不到原因, 感覺純粹是實驗結果好, 不然以論文中所說, 應該要盡量用抽象資訊才對\n",
    "# Q: 為何只取一個block不取全部?\n",
    "# A: 論文貌似也沒說明, 感覺上是因為這層足夠抽象(但又回到為啥不取block5_conv4了QQ\")\n",
    "content_feature_layers = ['block5_conv2']\n",
    "# Q: 為何都用conv1?\n",
    "# A: 因為我們通常會認為較淺層保留偏向局部的資訊, 而較深層含有更抽象的資訊\n",
    "#    因此我們也認為淺層比較能夠代表style (我猜的)\n",
    "style_feature_layers = ['block1_conv1', 'block2_conv1',\n",
    "                       'block3_conv1', 'block4_conv1',\n",
    "                       'block5_conv1']\n",
    "\n",
    "content_features = [outputs_dict[layer] for layer in content_feature_layers]\n",
    "style_features = [outputs_dict[layer] for layer in style_feature_layers]\n",
    "\n",
    "# 輸入input給model, 得到指定層的output(這些output根據輸入而變化)\n",
    "get_content_fun = K.function([static_model.input], content_features)\n",
    "get_style_fun = K.function([static_model.input], style_features)\n",
    "# 輸入content和style, 分別得到想要的層的結果, 用來算loss\n",
    "content_targets = get_content_fun([base_img_arr])\n",
    "style_targets = get_style_fun([ref_img_arr])\n",
    "\n",
    "# 記住, 輸出的值要轉成variable才能計算gradients\n",
    "content_targets_dict = {k: K.variable(v) for k, v in zip(content_feature_layers, content_targets)}\n",
    "style_targets_dict = {k: K.variable(v) for k, v in zip(style_feature_layers, style_targets)}\n",
    "# 這是要訓練的NN\n",
    "trainable_model = vgg19.VGG19(weights='imagenet', include_top=False,\n",
    "                              input_tensor=Input(tensor=generated_img_var))\n",
    "generated_outputs_dict = {layer.name: layer.output for layer in trainable_model.layers}\n",
    "\n",
    "loss = K.variable(0.)\n",
    "# add content loss\n",
    "for layer_name in content_feature_layers:\n",
    "    layer_feature = content_targets_dict[layer_name][0]\n",
    "    gen_feature = generated_outputs_dict[layer_name][0]\n",
    "    l = content_loss(layer_feature, gen_feature)\n",
    "    loss = loss + content_weight/len(content_feature_layers) * l\n",
    "print(loss)\n",
    "# add style loss\n",
    "for layer_name in style_feature_layers:\n",
    "    layer_feature = style_targets_dict[layer_name][0]\n",
    "    gen_feature = generated_outputs_dict[layer_name][0]\n",
    "    l = style_loss(layer_feature, gen_feature)\n",
    "    # 前面所提的w_l簡易的選個值做平均分配\n",
    "    loss = loss + style_weight / len(style_feature_layers) * l\n",
    "# add tv loss\n",
    "loss = loss + total_variation_weight * total_variation_loss(generated_img_var[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Nadam(20)\n",
    "updates = opt.get_updates([generated_img_var], {}, loss)\n",
    "# List of outputs\n",
    "outputs = [loss]\n",
    "# Function that makes a step after backpropping to the image\n",
    "make_step = K.function([], outputs, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total loss at pass10: 1.01131e+09\n",
      " Total loss at pass20: 7.56509e+08\n"
     ]
    }
   ],
   "source": [
    "max_iters = 1000\n",
    "for i in range(max_iters):\n",
    "    out = make_step([])\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(' Total loss at pass{}: %g'.format(i+1) %out[-1])\n",
    "        x = K.get_value(generated_img_var)        \n",
    "        fname = 'results/{}.png'.format(i+1)\n",
    "        img = deprocess_image(x)\n",
    "        img = Image.fromarray(img)\n",
    "        img.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
